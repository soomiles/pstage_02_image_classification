{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sound-volume",
   "metadata": {},
   "source": [
    "# Lesson 7 & 8 - Training & Inference\n",
    "- 7강과 8강에서는 모델을 학습하고 추론하는 방법에 대해 알아보았습니다.\n",
    "- 이번 실습 자료에서는 다양한 Loss, Optimizer, Scheduler를 활용하는 방법을 알아봅니다.\n",
    "- 또한, Checkpoint, Early Stopping과 같은 학습을 도와주는 Callback 방법을 알아봅니다.\n",
    "- 그리고 Graident Accumulation 방법을 활용하여 학습을 진행해봅니다.\n",
    "## 0. Libraries & Configurations\n",
    "- 시각화에 필요한 라이브러리와 학습에 필요한 설정을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "infrared-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os, sys\n",
    "from importlib import import_module\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import SGD, Adam, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from dataset import MaskBaseDataset\n",
    "from model import *\n",
    "\n",
    "sys.path.append('../')\n",
    "from dataset import MaskMultiClassDataset\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    동일한 조건으로 학습을 할 때, 동일한 결과를 얻기 위해 seed를 고정시킵니다.\n",
    "    \n",
    "    Args:\n",
    "        seed: seed 정수값\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latter-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- parameters\n",
    "img_root = '/mnt/ssd/data/mask_final/train/images'  # 학습 이미지 폴더의 경로\n",
    "label_path = '/mnt/ssd/data/mask_final/train/train.csv'  # 학습 메타파일의 경로\n",
    "\n",
    "model_name = \"VGG19\"  # 모델 이름\n",
    "use_pretrained = True  # pretrained-model의 사용 여부\n",
    "freeze_backbone = False  # classifier head 이 외 부분을 업데이트되지 않게 할 것인지 여부\n",
    "\n",
    "val_split = 0.4  # validation dataset의 비율\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "num_classes = 18\n",
    "\n",
    "num_epochs = 5  # 학습할 epoch의 수\n",
    "lr = 1e-4\n",
    "lr_decay_step = 10\n",
    "\n",
    "train_log_interval = 20  # logging할 iteration의 주기\n",
    "name = \"02_vgg\"  # 결과를 저장하는 폴더의 이름\n",
    "\n",
    "# -- settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-appeal",
   "metadata": {},
   "source": [
    "## 1. Loss\n",
    "- Image Classification에 사용되는 다양한 loss 함수들이 존재합니다. 각 loss 함수는 목적이 있고 풀고자 하는 문제에 맞게 적용을 해야합니다.\n",
    "- Cross Entropy Loss는 두 분포간의 불확실성을 최소화 하는 목적을 가진 분류에 사용되는 일반적인 손실함수입니다.\n",
    "- Focal Loss는 Imbalanced Data 문제를 해결하기 위한 손실함수입니다. [참고](https://arxiv.org/pdf/1708.02002.pdf)\n",
    "- Label Smoothing은 학습 데이터의 representation을 더 잘나타내는데 도움을 줍니다. [참고](https://arxiv.org/pdf/1906.02629.pdf)\n",
    "- F1 Loss는 F1 score 향상을 목적으로 하는 손실함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "closed-launch",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- Cross Entropy Loss\n",
    "class CrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "filled-configuration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- Focal Loss\n",
    "# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None,\n",
    "                 gamma=2., reduction='mean'):\n",
    "        nn.Module.__init__(self)\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input_tensor, target_tensor):\n",
    "        log_prob = F.log_softmax(input_tensor, dim=-1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        return F.nll_loss(\n",
    "            ((1 - prob) ** self.gamma) * log_prob,\n",
    "            target_tensor,\n",
    "            weight=self.weight,\n",
    "            reduction=self.reduction\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "informative-banks",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- Label Smoothing Loss\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "desirable-accounting",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -- F1 Loss\n",
    "# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n",
    "class F1Loss(nn.Module):\n",
    "    def __init__(self, classes=3, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        self.epsilon = epsilon\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.ndim == 2\n",
    "        assert y_true.ndim == 1\n",
    "        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
    "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
    "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n",
    "        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n",
    "        return 1 - f1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reliable-forty",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-picking",
   "metadata": {},
   "source": [
    "## 2. Optimizer\n",
    "- 파이토치는 코드를 간단히 수정하여 다양한 optimizer를 사용할 수 있습니다.\n",
    "- 또한 Model의 레이어마다 다른 learning rate를 적용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stupid-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- model\n",
    "model_cls = getattr(import_module(\"model\"), model_name)\n",
    "model = model_cls(\n",
    "    num_classes=num_classes,\n",
    "    pretrained=use_pretrained,\n",
    "    freeze=freeze_backbone\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "native-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- SGD optimizer\n",
    "optimizer = SGD(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opposed-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Adam optimizer\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-touch",
   "metadata": {},
   "source": [
    "- 한 모델에 다른 learning rate를 적용시키기 위해 모델의 구조를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elect-partner",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('net',\n",
       "  VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU(inplace=True)\n",
       "      (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (16): ReLU(inplace=True)\n",
       "      (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (19): ReLU(inplace=True)\n",
       "      (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (32): ReLU(inplace=True)\n",
       "      (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (38): ReLU(inplace=True)\n",
       "      (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (42): ReLU(inplace=True)\n",
       "      (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (45): ReLU(inplace=True)\n",
       "      (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (48): ReLU(inplace=True)\n",
       "      (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (51): ReLU(inplace=True)\n",
       "      (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=18, bias=True)\n",
       "    )\n",
       "  ))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proof-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- optimizer: Different Learning Rates on different layers\n",
    "\n",
    "# features 레이어와 classifier 레이어에서 서로 다른 learning rate를 적용하여 optimizer를 정의할 수 있습니다.\n",
    "train_params = [{'params': getattr(model.net, 'features').parameters(), 'lr': lr / 10, 'weight_decay':5e-4},\n",
    "                {'params': getattr(model.net, 'classifier').parameters(), 'lr': lr, 'weight_decay':5e-4}]\n",
    "optimizer = Adam(train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-merchandise",
   "metadata": {},
   "source": [
    "## 3. Scheduler\n",
    "- Scheduler은 optimizer의 learning rate를 동적으로 변경시키는 기능을 합니다.\n",
    "- Optimizer과 Scheduler를 적절히 활용하면 모델이 좋은 성능으로 Fitting하는데 도움을 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "original-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: StepLR\n",
    "# 지정된 step마다 learning rate를 감소시킵니다.\n",
    "scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "resistant-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: ReduceLROnPlateau\n",
    "# 성능이 향상되지 않을 때 learning rate를 줄입니다. patience=10은 10회 동안 성능 향상이 없을 경우입니다.\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "distant-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scheduler: CosineAnnealingLR\n",
    "# CosineAnnealing은 learning rate를 cosine 그래프처럼 변화시킵니다.\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-documentary",
   "metadata": {},
   "source": [
    "## 4. Metric\n",
    "- Classification 성능을 표현할 때 다양한 평가지표가 있습니다.\n",
    "- Accuracy: 모델이 정확하게 예측한 객체의 비율\n",
    "- True Positive(TP): 실제 True인 정답을 True라고 예측 (정답)\n",
    "- False Positive(FP): 실제 False인 정답을 True라고 예측 (오답)\n",
    "- False Negative(FN): 실제 True인 정답을 False라고 예측 (오답)\n",
    "- True Negative(TN): 실제 False인 정답을 False라고 예측 (정답)\n",
    "- Precision(정밀도): TP / (TP + FP)\n",
    "- Recall(재현율): TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "racial-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "union-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Accuracy\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "rotary-working",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Accuracy\n",
    "# Normalize를 안하면 맞춘 개수가 표시된다\n",
    "accuracy_score(y_true, y_pred, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "employed-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Precision\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "designed-storage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- Recall\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "informed-consolidation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- f1 score\n",
    "2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "owned-victory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- f1 score (sklearn)\n",
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-guess",
   "metadata": {},
   "source": [
    "## 5. Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "several-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaskMultiClassDataset(img_root, label_path, 'train')\n",
    "n_val = int(len(dataset) * val_split)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "val_set.dataset.set_phase(\"test\")  # todo : fix\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-moisture",
   "metadata": {},
   "source": [
    "### 5.1 Callback - Checkpoint, Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "insured-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Callback1: Checkpoint - Accuracy가 높아질 때마다 모델을 저장합니다.\n",
    "# 학습 코드에서 이어집니다.\n",
    "\n",
    "# -- Callback2: Early Stopping - 성능이 일정 기간동안 향상이 없을 경우 학습을 종료합니다.\n",
    "patience = 10\n",
    "counter = 0\n",
    "# 학습 코드에서 이어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-citizenship",
   "metadata": {},
   "source": [
    "### 5.2 Training Method - Gradient Accumulation\n",
    "- Graident Accumulation은 한 iteration에 파라미터를 업데이트시키는게 아니라, gradient를 여러 iteration 동안 쌓아서 업데이트시킵니다. 한 번에 파라미터를 업데이트시키는 건 noise가 있을 수 있으므로, 여러번 쌓아서 한번에 업데이트 시킴으로써 그러한 문제를 방지하기 위함입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bibliographic-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Gradient Accumulation\n",
    "accumulation_steps = 2\n",
    "# 학습코드에서 이어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-poker",
   "metadata": {},
   "source": [
    "### 5.3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "appointed-nirvana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/5](20/169) || training loss 2.261 || training accuracy 33.98% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](40/169) || training loss 1.641 || training accuracy 51.41% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](60/169) || training loss 1.269 || training accuracy 62.34% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](80/169) || training loss 1.038 || training accuracy 66.72% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](100/169) || training loss 0.9232 || training accuracy 68.75% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](120/169) || training loss 0.8036 || training accuracy 73.83% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](140/169) || training loss 0.7067 || training accuracy 76.64% || lr [1e-05, 0.0001]\n",
      "Epoch[0/5](160/169) || training loss 0.7334 || training accuracy 77.03% || lr [1e-05, 0.0001]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 81.48%, loss: 0.54 || best acc : 81.48%, best loss: 0.54\n",
      "Epoch[1/5](20/169) || training loss 0.5276 || training accuracy 81.48% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](40/169) || training loss 0.4665 || training accuracy 86.09% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](60/169) || training loss 0.4224 || training accuracy 86.02% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](80/169) || training loss 0.4048 || training accuracy 86.64% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](100/169) || training loss 0.4033 || training accuracy 86.02% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](120/169) || training loss 0.3613 || training accuracy 87.58% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](140/169) || training loss 0.3226 || training accuracy 89.45% || lr [5e-06, 5e-05]\n",
      "Epoch[1/5](160/169) || training loss 0.3713 || training accuracy 87.50% || lr [5e-06, 5e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 84.52%, loss: 0.43 || best acc : 84.52%, best loss: 0.43\n",
      "Epoch[2/5](20/169) || training loss 0.3427 || training accuracy 87.19% || lr [0.0, 0.0]\n",
      "Epoch[2/5](40/169) || training loss 0.2983 || training accuracy 90.23% || lr [0.0, 0.0]\n",
      "Epoch[2/5](60/169) || training loss 0.3161 || training accuracy 89.30% || lr [0.0, 0.0]\n",
      "Epoch[2/5](80/169) || training loss 0.3014 || training accuracy 90.39% || lr [0.0, 0.0]\n",
      "Epoch[2/5](100/169) || training loss 0.2884 || training accuracy 89.61% || lr [0.0, 0.0]\n",
      "Epoch[2/5](120/169) || training loss 0.2581 || training accuracy 91.41% || lr [0.0, 0.0]\n",
      "Epoch[2/5](140/169) || training loss 0.2425 || training accuracy 92.58% || lr [0.0, 0.0]\n",
      "Epoch[2/5](160/169) || training loss 0.2725 || training accuracy 90.86% || lr [0.0, 0.0]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 84.60%, loss: 0.43 || best acc : 84.60%, best loss: 0.43\n",
      "Epoch[3/5](20/169) || training loss 0.3584 || training accuracy 87.50% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/5](40/169) || training loss 0.29 || training accuracy 89.30% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/5](60/169) || training loss 0.2548 || training accuracy 90.55% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/5](80/169) || training loss 0.2529 || training accuracy 92.27% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/5](100/169) || training loss 0.2676 || training accuracy 90.94% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/5](120/169) || training loss 0.2268 || training accuracy 91.95% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/5](140/169) || training loss 0.184 || training accuracy 93.36% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Epoch[3/5](160/169) || training loss 0.2219 || training accuracy 91.88% || lr [4.9999999999999996e-06, 4.9999999999999996e-05]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 86.68%, loss: 0.38 || best acc : 86.68%, best loss: 0.38\n",
      "Epoch[4/5](20/169) || training loss 0.2221 || training accuracy 91.95% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/5](40/169) || training loss 0.2014 || training accuracy 92.66% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/5](60/169) || training loss 0.1884 || training accuracy 93.36% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/5](80/169) || training loss 0.1844 || training accuracy 93.67% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/5](100/169) || training loss 0.1778 || training accuracy 92.97% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/5](120/169) || training loss 0.1602 || training accuracy 93.91% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/5](140/169) || training loss 0.1311 || training accuracy 95.47% || lr [1e-05, 0.00010000000000000002]\n",
      "Epoch[4/5](160/169) || training loss 0.1261 || training accuracy 94.61% || lr [1e-05, 0.00010000000000000002]\n",
      "Calculating validation results...\n",
      "New best model for val accuracy! saving the model..\n",
      "[Val] acc : 88.79%, loss: 0.35 || best acc : 88.79%, best loss: 0.35\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n",
    "\n",
    "counter = 0\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    # train loop\n",
    "    model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    for idx, train_batch in enumerate(train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outs = model(inputs)\n",
    "        preds = torch.argmax(outs, dim=-1)\n",
    "        loss = criterion(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # -- Gradient Accumulation\n",
    "        if (idx+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % train_log_interval == 0:\n",
    "            train_loss = loss_value / train_log_interval\n",
    "            train_acc = matches / batch_size / train_log_interval\n",
    "            current_lr = scheduler.get_last_lr()\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"Calculating validation results...\")\n",
    "        model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        for val_batch in val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = criterion(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(val_set)\n",
    "        \n",
    "        # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        if val_acc > best_val_acc:\n",
    "            print(\"New best model for val accuracy! saving the model..\")\n",
    "            torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n",
    "        if counter > patience:\n",
    "            print(\"Early Stopping...\")\n",
    "            break\n",
    "        \n",
    "        \n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-baltimore",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "alert-marathon",
   "metadata": {},
   "source": [
    "## 6. Reference\n",
    "- [sumni blog post](https://sumniya.tistory.com/26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-workshop",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
